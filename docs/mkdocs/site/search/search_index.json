{
  "config": {
    "lang": [
      "en"
    ],
    "separator": "[\\s\\-]+",
    "pipeline": [
      "stopWordFilter"
    ]
  },
  "docs": [
    {
      "location": "",
      "title": "Team 5152 Robot Code Documentation",
      "text": "<p>Welcome to the comprehensive documentation for Team 5152's robot code. This documentation provides an in-depth look at our robot's sophisticated control and automation systems.</p>"
    },
    {
      "location": "#core-systems",
      "title": "Core Systems",
      "text": "<p>Our robot's drive system is built around an advanced SwerveDrive Subsystem that provides precise omnidirectional control. This integrates seamlessly with our Controls System, which handles driver input processing and sophisticated deadband management. The drive system also incorporates autonomous path following and trajectory generation capabilities for consistent, repeatable performance.</p> <p>Vision processing forms another cornerstone of our robot's capabilities. The AprilTag System provides field-relative pose estimation, while our Object Detection system enables real-time game piece tracking. These systems work together through multi-camera fusion and filtering to maintain accurate spatial awareness during matches.</p>"
    },
    {
      "location": "#game-specific-features",
      "title": "Game-Specific Features",
      "text": "<p>We've developed several specialized commands to enhance game performance. The DriveFacingBestObject command provides assisted game piece targeting, while PathfindToBestObject enables autonomous navigation to game pieces. Our DefaultSetToAllianceColor command manages alliance-specific indicators.</p>"
    },
    {
      "location": "#support-infrastructure",
      "title": "Support Infrastructure",
      "text": "<p>Supporting these primary systems, our robot includes comprehensive Pneumatics Control for actuator and pressure management, along with an LED Notification System that provides clear visual feedback about robot state and debugging information.</p>"
    },
    {
      "location": "#developer-resources",
      "title": "Developer Resources",
      "text": "<p>For detailed technical information, developers can access our JavaDoc API Documentation and review our source code directly on GitHub. Our codebase emphasizes modular design through a command-based architecture, making it easy to extend and maintain. The system includes comprehensive telemetry and debugging capabilities, ensuring smooth development and testing processes.</p>"
    },
    {
      "location": "game/subsystems/controls/",
      "title": "Controls Subsystem",
      "text": "<p>The controls subsystem handles driver input processing and mapping. It provides sophisticated input processing for precise robot control.</p> <p>This system integrates with: - SwerveDrive Subsystem for robot movement - DriveFacingBestObject for assisted driving - PathfindToBestObject for autonomous navigation</p>"
    },
    {
      "location": "game/subsystems/controls/#input-processing-features",
      "title": "Input Processing Features",
      "text": ""
    },
    {
      "location": "game/subsystems/controls/#deadband-processing",
      "title": "Deadband Processing",
      "text": "<p>The <code>JoystickUtilities</code> class provides several input processing methods:</p> <ul> <li><code>joyDeadBnd</code>: Applies deadband with smooth transition</li> <li><code>joySqrd</code>: Square input for finer control</li> <li><code>joyScaled</code>: Scale input by a factor</li> </ul>"
    },
    {
      "location": "game/subsystems/controls/#configuration",
      "title": "Configuration",
      "text": ""
    },
    {
      "location": "game/subsystems/controls/#deadband-constants",
      "title": "Deadband Constants",
      "text": "<p>Located in <code>HMIDeadbands</code>: - Forward/Backward: 0.1 - Strafe: 0.1</p>"
    },
    {
      "location": "game/subsystems/controls/#usage-examples",
      "title": "Usage Examples",
      "text": "<pre><code>// Apply deadband to raw joystick input\ndouble processed = JoystickUtilities.joyDeadBnd(rawInput, HMIDeadbands.DRIVER_FWD_AXIS_DEADBAND);\n\n// Square input for more precise control\ndouble squared = JoystickUtilities.joySqrd(processed);\n\n// Scale input\ndouble scaled = JoystickUtilities.joyScaled(processed, 0.5);\n</code></pre>"
    },
    {
      "location": "game/subsystems/controls/#best-practices",
      "title": "Best Practices",
      "text": "<ol> <li>Always use deadband processing for joystick inputs</li> <li>Consider using squared inputs for fine control</li> <li>Test control sensitivity with different drivers</li> </ol>"
    },
    {
      "location": "library/commands/defaultsettoalliancecolor/",
      "title": "DefaultSetToAllianceColor Command",
      "text": ""
    },
    {
      "location": "library/commands/defaultsettoalliancecolor/#purpose",
      "title": "Purpose",
      "text": "<p>DefaultSetToAllianceColor serves as the default command for the Bling subsystem, maintaining the robot's LED indicators at the current alliance color. This command runs continuously, including when the robot is disabled, ensuring consistent visual indication of alliance membership throughout a match.</p>"
    },
    {
      "location": "library/commands/defaultsettoalliancecolor/#required-subsystems",
      "title": "Required Subsystems",
      "text": "<ul> <li>Bling Subsystem</li> </ul>"
    },
    {
      "location": "library/commands/defaultsettoalliancecolor/#constructor",
      "title": "Constructor",
      "text": "<pre><code>public DefaultSetToAllianceColor(BlingSubsystem subSysBling)\n</code></pre>"
    },
    {
      "location": "library/commands/defaultsettoalliancecolor/#operation-details",
      "title": "Operation Details",
      "text": "<p>The command operates with minimal complexity, setting the LED state during initialization and then maintaining that state. The LED updates are handled through the subsystem's periodic method, requiring no additional execution logic in the command itself.</p> <p>The command is designed to run indefinitely and continue operation even when the robot is disabled. This behavior ensures the alliance color remains visible during all robot states, including pre-match setup and post-match periods.</p>"
    },
    {
      "location": "library/commands/defaultsettoalliancecolor/#special-characteristics",
      "title": "Special Characteristics",
      "text": "<p>The command implements several important behaviors: - Runs when disabled through the <code>runsWhenDisabled()</code> override - Never completes execution (<code>isFinished()</code> returns false) - Requires minimal resources as primary updates occur in the subsystem - Maintains LED state without continuous commanding</p>"
    },
    {
      "location": "library/commands/defaultsettoalliancecolor/#usage-example",
      "title": "Usage Example",
      "text": "<pre><code>BlingSubsystem bling = new BlingSubsystem();\nbling.setDefaultCommand(new DefaultSetToAllianceColor(bling));\n</code></pre> <p>This command is typically set as the default command for the Bling subsystem during robot initialization, ensuring the alliance color display operates as the baseline behavior when no other LED commands are running.</p>"
    },
    {
      "location": "library/commands/drivefacingbestobject/",
      "title": "DriveFacingBestObject Command",
      "text": ""
    },
    {
      "location": "library/commands/drivefacingbestobject/#purpose",
      "title": "Purpose",
      "text": "<p>The DriveFacingBestObject command enables field-centric robot driving while automatically orienting the robot to face detected game pieces. It combines manual translation control with automated rotation management, providing smooth transitions between automated facing and manual override capabilities.</p> <p>This command works in conjunction with the PhotonVision Object Detection System and SwerveDrive Subsystem.</p> <p>See also: PathfindToBestObject for autonomous navigation to objects.</p>"
    },
    {
      "location": "library/commands/drivefacingbestobject/#required-subsystems",
      "title": "Required Subsystems",
      "text": "<ul> <li>PhotonVision Object Detection System</li> <li>SwerveDrive Subsystem</li> </ul>"
    },
    {
      "location": "library/commands/drivefacingbestobject/#constructor",
      "title": "Constructor",
      "text": "<pre><code>public DriveFacingBestObject(\n    PhotonVisionObjectDetectionSubsystem objectDetectionSubsystem,\n    SwerveDriveSubsystem swerveDriveSubsystem,\n    DoubleSupplier velocityX,\n    DoubleSupplier velocityY,\n    DoubleSupplier velocityRotation\n)\n</code></pre> <p>The constructor accepts suppliers for velocity control, allowing flexible input sources such as joysticks or programmatic values. The velocity inputs control field-centric movement while the command manages rotation automatically.</p>"
    },
    {
      "location": "library/commands/drivefacingbestobject/#operation-modes",
      "title": "Operation Modes",
      "text": ""
    },
    {
      "location": "library/commands/drivefacingbestobject/#object-detected-mode",
      "title": "Object Detected Mode",
      "text": "<p>When objects are detected, the command: - Maintains field-centric translation using manual X/Y inputs - Automatically rotates to face the highest-confidence detected object - Uses a PID controller (P=5.0) for rotation control</p>"
    },
    {
      "location": "library/commands/drivefacingbestobject/#no-detection-mode",
      "title": "No Detection Mode",
      "text": "<p>When no objects are detected, the command: - Maintains field-centric translation using manual X/Y inputs - Allows manual rotation control through the velocityRotation supplier - Operates as a standard field-centric drive command</p>"
    },
    {
      "location": "library/commands/drivefacingbestobject/#manual-override",
      "title": "Manual Override",
      "text": "<p>The command features a brief manual override capability: - Override activates when manual rotation input is detected - Override timeout of 0.1 seconds - Command ends when override timeout elapses</p>"
    },
    {
      "location": "library/commands/drivefacingbestobject/#configuration",
      "title": "Configuration",
      "text": "<p>Key parameters that may need tuning:</p> <pre><code>// Rotation control PID values\ndriveFacingAngle.HeadingController = new PhoenixPIDController(5.0, 0, 0.0);\n\n// Override timeout duration\nprivate static final double OVERRIDE_TIMEOUT_SECONDS = 0.1;\n</code></pre>"
    },
    {
      "location": "library/commands/drivefacingbestobject/#usage-example",
      "title": "Usage Example",
      "text": "<pre><code>new DriveFacingBestObject(\n    visionSubsystem,\n    driveSubsystem,\n    () -&gt; -driverController.getLeftY(),   // Forward/back\n    () -&gt; -driverController.getLeftX(),   // Left/right\n    () -&gt; -driverController.getRightX()   // Manual rotation override\n);\n</code></pre> <p>This command is particularly useful for game piece acquisition sequences where the driver needs to approach detected objects while maintaining optimal orientation.</p>"
    },
    {
      "location": "library/commands/pathfindtobestobject/",
      "title": "PathfindToBestObject Command",
      "text": ""
    },
    {
      "location": "library/commands/pathfindtobestobject/#purpose",
      "title": "Purpose",
      "text": "<p>The PathfindToBestObject command creates and executes a path for the robot to navigate to the closest detected game piece while maintaining a safe approach distance based on the robot's bumper dimensions. This instant command generates and schedules a pathfinding command that considers the robot's physical dimensions and optimal approach angles.</p> <p>Related features: - DriveFacingBestObject for manual driving toward objects - Object Detection System for vision processing - SwerveDrive for path following</p>"
    },
    {
      "location": "library/commands/pathfindtobestobject/#required-subsystems",
      "title": "Required Subsystems",
      "text": "<ul> <li>PhotonVision Object Detection System</li> <li>SwerveDrive Subsystem</li> <li>SwerveDrivePathPlanner</li> </ul>"
    },
    {
      "location": "library/commands/pathfindtobestobject/#constructor",
      "title": "Constructor",
      "text": "<pre><code>public PathfindToBestObject(\n    PhotonVisionObjectDetectionSubsystem objectDetectionSubsystem,\n    SwerveDriveSubsystem swerveDriveSubsystem,\n    SwerveDrivePathPlanner pathPlanner\n)\n</code></pre>"
    },
    {
      "location": "library/commands/pathfindtobestobject/#operation-details",
      "title": "Operation Details",
      "text": "<p>The command performs several key calculations during initialization:</p> <ol> <li>Determines the best detected object from the vision system</li> <li>Calculates the approach vector from robot to object</li> <li>Selects appropriate bumper offset based on approach angle</li> <li>Generates a target pose offset from the object for safe approach</li> <li>Creates and schedules a PathPlanner command to the calculated position</li> </ol>"
    },
    {
      "location": "library/commands/pathfindtobestobject/#smart-approach-system",
      "title": "Smart Approach System",
      "text": "<p>The command implements intelligent approach positioning by: - Using the robot's actual bumper dimensions (length and width) - Dynamically selecting the appropriate bumper dimension based on approach angle - Calculating an offset position that prevents collision with the target object</p>"
    },
    {
      "location": "library/commands/pathfindtobestobject/#usage-example",
      "title": "Usage Example",
      "text": "<pre><code>new PathfindToBestObject(\n    visionSubsystem,\n    driveSubsystem,\n    pathPlanner\n).schedule();\n</code></pre>"
    },
    {
      "location": "library/commands/pathfindtobestobject/#important-notes",
      "title": "Important Notes",
      "text": "<ul> <li>The command executes instantly but schedules a longer-running path following command</li> <li>Returns immediately if no objects are detected</li> <li>The target pose maintains the object's rotation</li> <li>Sets terminal velocity to 0 m/s for safe approach</li> <li>Requires accurate bumper dimensions in TunerConstants</li> </ul> <p>The command is particularly useful for autonomous sequences and driver assistance features where precise positioning near game pieces is required.</p>"
    },
    {
      "location": "library/subsystems/bling/",
      "title": "Bling Subsystem",
      "text": ""
    },
    {
      "location": "library/subsystems/bling/#overview",
      "title": "Overview",
      "text": "<p>The BlingSubsystem controls LED lighting (Bling) on the robot using a CTRE CANdle controller. This subsystem provides comprehensive LED control capabilities, including solid colors, animations, queued patterns, and alliance-based color schemes. The system is designed to provide visual feedback about robot state and enhance field presence.</p>"
    },
    {
      "location": "library/subsystems/bling/#core-features",
      "title": "Core Features",
      "text": "<p>The subsystem provides robust LED control through several key mechanisms:</p>"
    },
    {
      "location": "library/subsystems/bling/#color-management",
      "title": "Color Management",
      "text": "<p>The system supports immediate and queued color changes:</p> <pre><code>// Set immediate color\nbling.setSolidColor(Color.RED);\n\n// Queue next color\nbling.queueColor(nextColor);\nbling.setQueuedColor();  // Apply when ready\n</code></pre>"
    },
    {
      "location": "library/subsystems/bling/#animation-control",
      "title": "Animation Control",
      "text": "<p>Animations can be controlled directly or through a queue system:</p> <pre><code>// Run animation immediately\nbling.runAnimation(new RainbowAnimation());\n\n// Queue animation\nbling.queueAnimation(nextAnimation);\nbling.runQueuedAnimation();  // Apply when ready\n</code></pre>"
    },
    {
      "location": "library/subsystems/bling/#alliance-integration",
      "title": "Alliance Integration",
      "text": "<p>The system automatically handles alliance colors:</p> <pre><code>bling.setLedToAllianceColor();  // Sets red/blue based on alliance\n</code></pre>"
    },
    {
      "location": "library/subsystems/bling/#configuration",
      "title": "Configuration",
      "text": ""
    },
    {
      "location": "library/subsystems/bling/#hardware-setup",
      "title": "Hardware Setup",
      "text": "<p>The system expects: - CTRE CANdle controller - GRB LED strip type - Configurable number of LEDs (default 92) - Optional LED offset (default 8)</p>"
    },
    {
      "location": "library/subsystems/bling/#tunable-constants",
      "title": "Tunable Constants",
      "text": "<p>Key parameters in BlingSubsystemConstants:</p> <pre><code>public static final boolean BLING_ENABLED = false;\npublic static final double MAX_LED_BRIGHTNESS = 0.25;\npublic static final int NUM_LEDS = 92;\npublic static final int LED_OFFSET = 8;\n</code></pre>"
    },
    {
      "location": "library/subsystems/bling/#telemetry",
      "title": "Telemetry",
      "text": "<p>The subsystem includes comprehensive telemetry through Shuffleboard: - Current color display - Active animation status - System enable/disable status</p>"
    },
    {
      "location": "library/subsystems/bling/#best-practices",
      "title": "Best Practices",
      "text": ""
    },
    {
      "location": "library/subsystems/bling/#state-management",
      "title": "State Management",
      "text": "<p>The subsystem maintains clear state through dedicated methods: - <code>clearAnimation()</code> for stopping animations - <code>clearSolidColor()</code> for turning off LEDs - <code>clearAll()</code> for complete reset - <code>runDefault()</code> for returning to baseline behavior</p>"
    },
    {
      "location": "library/subsystems/bling/#update-cycle",
      "title": "Update Cycle",
      "text": "<p>The subsystem handles updates automatically through: - Periodic updates in the subsystem - Automatic telemetry updates - State management in the command loop</p>"
    },
    {
      "location": "library/subsystems/bling/#integration-notes",
      "title": "Integration Notes",
      "text": "<p>To add this subsystem to your robot:</p> <ol> <li>Configure CAN ID for the CANdle</li> <li>Adjust constants for your LED strip configuration</li> <li>Set up default command (typically DefaultSetToAllianceColor)</li> <li>Integrate with robot state indicators as needed</li> </ol> <p>This subsystem is designed to be both robust for competition use and flexible for development and testing purposes.</p>"
    },
    {
      "location": "library/subsystems/pneumatics/",
      "title": "Pneumatics Subsystem",
      "text": "<p>The pneumatics subsystem handles all pneumatic actuators on the robot. This includes:</p>"
    },
    {
      "location": "library/subsystems/pneumatics/#features",
      "title": "Features",
      "text": "<ul> <li>Solenoid control</li> <li>Pressure monitoring</li> <li>Compressor management</li> </ul>"
    },
    {
      "location": "library/subsystems/pneumatics/#configuration",
      "title": "Configuration",
      "text": "<p>The pneumatics system is configured through the <code>PneumaticsSubsystemConstants</code> class.</p>"
    },
    {
      "location": "library/subsystems/pneumatics/#best-practices",
      "title": "Best Practices",
      "text": "<ol> <li>Always monitor system pressure</li> <li>Use double solenoids when possible for positive control</li> <li>Implement safety timeouts for actuations</li> </ol>"
    },
    {
      "location": "library/subsystems/swerve/",
      "title": "SwerveDrive Subsystem",
      "text": ""
    },
    {
      "location": "library/subsystems/swerve/#system-overview",
      "title": "System Overview",
      "text": "<p>The SwerveDrive system consists of multiple interconnected components that work together to provide a complete swerve drive implementation for FRC robots. The system is built on CTRE's Phoenix 6 framework and integrates with PathPlanner for autonomous path following.</p>"
    },
    {
      "location": "library/subsystems/swerve/#core-components",
      "title": "Core Components",
      "text": ""
    },
    {
      "location": "library/subsystems/swerve/#1-swervedrivesubsystem",
      "title": "1. SwerveDriveSubsystem",
      "text": "<p>The main subsystem class that handles drive control and odometry. This class extends CTRE's <code>SwerveDrivetrain</code> and implements WPILib's <code>Subsystem</code> interface.</p>"
    },
    {
      "location": "library/subsystems/swerve/#2-swervedrivepathplanner",
      "title": "2. SwerveDrivePathPlanner",
      "text": "<p>Manages autonomous path following and path generation capabilities. This includes auto path selection and execution.</p>"
    },
    {
      "location": "library/subsystems/swerve/#3-tunerconstants",
      "title": "3. TunerConstants",
      "text": "<p>Year-specific configuration files (mk4il22023 and mk4il32024) that contain hardware-specific constants and module configurations.</p>"
    },
    {
      "location": "library/subsystems/swerve/#4-swervedrivetelemetry",
      "title": "4. SwerveDriveTelemetry",
      "text": "<p>Handles data visualization and debugging through Shuffleboard.</p>"
    },
    {
      "location": "library/subsystems/swerve/#5-swervedrivesubsystemconstants",
      "title": "5. SwerveDriveSubsystemConstants",
      "text": "<p>Contains performance-related constants for different drive modes.</p>"
    },
    {
      "location": "library/subsystems/swerve/#configuration-guide",
      "title": "Configuration Guide",
      "text": ""
    },
    {
      "location": "library/subsystems/swerve/#module-configuration",
      "title": "Module Configuration",
      "text": "<p>Module configuration is handled in the year-specific TunerConstants files. To configure for your robot:</p> <ol> <li>Choose the appropriate year's TunerConstants file:<ul> <li><code>mk4il22023/TunerConstants.java</code> for L2 robot (2023)</li> <li><code>mk4il32024/TunerConstants.java</code> for L3 robot (2024)</li> </ul> </li> <li> <p>Or, generate a new one via PhoenixTunerX</p> </li> <li> <p>Update the following constants in your chosen TunerConstants file:</p> </li> </ol> <pre><code>// PID Gains\nprivate static final Slot0Configs steerGains = new Slot0Configs()\n    .withKP(100)  // Adjust based on your robot's steering response\n    .withKI(0)    \n    .withKD(0.2);\n\nprivate static final Slot0Configs driveGains = new Slot0Configs()\n    .withKP(3)    // Adjust based on your robot's driving response\n    .withKI(0)\n    .withKD(0);\n</code></pre>"
    },
    {
      "location": "library/subsystems/swerve/#pathplanner-integration",
      "title": "PathPlanner Integration",
      "text": "<p>In SwerveDrivePathPlanner.java:</p> <ol> <li>Configure path constraints:</li> </ol> <pre><code>// In the corresponding year's TunerConstants.java\npublic static final PathConstraints PATHFINDING_CONSTRAINTS = new PathConstraints(\n    5.2,  // Max velocity (m/s)\n    3.5,  // Max acceleration (m/s²)\n    Units.degreesToRadians(540),  // Max angular velocity\n    Units.degreesToRadians(460)   // Max angular acceleration\n);\n</code></pre> <ol> <li>Configure the holonomic drive controller:</li> </ol> <pre><code>public static final PPHolonomicDriveController PP_HOLONOMIC_DRIVE_CONTROLLER = \n    new PPHolonomicDriveController(\n        new PIDConstants(2.4, 0, 0.015),  // Translation PID\n        new PIDConstants(7.8, 0, 0.015)   // Rotation PID\n    );\n</code></pre>"
    },
    {
      "location": "library/subsystems/swerve/#performance-tuning",
      "title": "Performance Tuning",
      "text": "<p>In SwerveDriveSubsystemConstants.java:</p> <ol> <li>Configure performance modes:</li> </ol> <pre><code>public static final class PerformanceModeDefault {\n    public static final double DRIVE_TRAIN_MAX_SPD = 3.5; // m/s\n    public static final double DRIVE_TRAIN_MAX_ACCELERATION = 2.0; // m/s²\n    public static final double DRIVE_TRAIN_MAX_ROT_SPD = 0.75 * 2 * Math.PI; // rad/s\n}\n</code></pre> <ol> <li>Configure acceleration limits:</li> </ol> <pre><code>public static final double DRIVE_XY_SPD_PERF_MODE_SW_FILTER_RATE = 8.0; // m/s/s\npublic static final double DRIVE_ROT_SPD_PERF_MODE_SW_FILTER_RATE = 4.0; // rad/s/s\n</code></pre>"
    },
    {
      "location": "library/subsystems/swerve/#telemetry-configuration",
      "title": "Telemetry Configuration",
      "text": "<p>In SwerveDriveTelemetry.java:</p> <ol> <li>Configure Shuffleboard layout:</li> </ol> <pre><code>private ShuffleboardLayout initializePoseList(SwerveDriveSubsystem swerveDrive) {\n    return driveTab\n        .getLayout(\"Pose\", BuiltInLayouts.kList)\n        .withSize(2, 3)\n        .withPosition(0, 0);\n}\n</code></pre> <ol> <li>Add custom telemetry data:</li> </ol> <pre><code>private void initializeOtherWidgets(SwerveDriveSubsystem swerveDrive) {\n    driveTab.addBoolean(\"Custom Metric\", () -&gt; /* your condition */);\n}\n</code></pre>"
    },
    {
      "location": "library/subsystems/swerve/#usage-examples",
      "title": "Usage Examples",
      "text": ""
    },
    {
      "location": "library/subsystems/swerve/#basic-drive-configuration",
      "title": "Basic Drive Configuration",
      "text": "<pre><code>// Create drivetrain using the appropriate year's TunerConstants\nSwerveDriveSubsystem swerve = TunerConstants.createDrivetrain();\n\n// Configure PathPlanner\nSwerveDrivePathPlanner pathPlanner = new SwerveDrivePathPlanner(swerve);\n</code></pre>"
    },
    {
      "location": "library/subsystems/swerve/#autonomous-path-following",
      "title": "Autonomous Path Following",
      "text": "<pre><code>// Load and run an autonomous path\nCommand autoPath = pathPlanner.getAutoPath(\"YourPathName\");\nautoPath.schedule();\n\n// Create a pathfinding command to a specific pose\nCommand pathfindCommand = pathPlanner.getPathFinderCommand(\n    new Pose2d(1, 1, new Rotation2d()),\n    MetersPerSecond.of(0)\n);\n</code></pre>"
    },
    {
      "location": "library/subsystems/swerve/#manual-drive-control",
      "title": "Manual Drive Control",
      "text": "<pre><code>swerve.applyRequest(() -&gt; \n    new SwerveRequest.FieldCentric()\n        .withVelocityX(joystickX)\n        .withVelocityY(joystickY)\n        .withRotation(joystickRotation)\n);\n</code></pre>"
    },
    {
      "location": "library/subsystems/swerve/#best-practices",
      "title": "Best Practices",
      "text": "<ol> <li> <p>Module Configuration</p> <ul> <li>Calibrate encoder offsets with the robot elevated and wheels pointing forward</li> <li>Verify motor and encoder IDs match physical hardware</li> <li>Double-check gear ratios against physical hardware</li> </ul> </li> <li> <p>PathPlanner Usage</p> <ul> <li>Keep path constraints within physical capabilities</li> <li>Test autonomous paths at reduced speeds first</li> <li>Use the PathPlanner GUI to visualize and verify paths</li> </ul> </li> <li> <p>Performance Tuning</p> <ul> <li>Start with conservative speed limits</li> <li>Gradually increase limits while monitoring stability</li> <li>Test all performance modes thoroughly</li> </ul> </li> <li> <p>Telemetry</p> <ul> <li>Monitor module states during testing</li> <li>Use field visualization to verify odometry</li> <li>Log relevant data for troubleshooting</li> </ul> </li> </ol>"
    },
    {
      "location": "library/subsystems/vision/apriltag/",
      "title": "PhotonVision AprilTag Subsystem",
      "text": ""
    },
    {
      "location": "library/subsystems/vision/apriltag/#system-architecture",
      "title": "System Architecture",
      "text": "<p>The PhotonVision AprilTag subsystem implements a robust vision-based pose estimation system using multiple cameras to track AprilTag fiducial markers. The system processes multiple camera feeds simultaneously, fuses their data, and produces highly accurate pose estimates through sophisticated filtering and smoothing algorithms.</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#core-components",
      "title": "Core Components",
      "text": "<p>The system is built around three main components that work together:</p> <p>The PhotonvisionAprilTagSubsystem serves as the primary controller, managing the vision pipeline and pose estimation process. It interfaces with multiple PhotonCamera instances, each configured with specific mounting offsets and orientations relative to the robot's center. The subsystem maintains a collection of PhotonPoseEstimator objects that handle the geometric calculations for each camera.</p> <p>The PhotonvisionAprilTagTelemetry component provides real-time visualization and debugging capabilities, managing the data flow to Shuffleboard for monitoring system performance and pose estimates.</p> <p>PhotonvisionAprilTagSubsystemConstants contains the configuration parameters that define camera positions, filtering thresholds, and system behavior settings.</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#system-configuration-and-tuning",
      "title": "System Configuration and Tuning",
      "text": ""
    },
    {
      "location": "library/subsystems/vision/apriltag/#camera-addition-and-configuration",
      "title": "Camera Addition and Configuration",
      "text": "<p>Adding new cameras to the system requires modifications to both the hardware setup and software configuration. Each camera needs proper physical mounting with known offsets from the robot's center point, and corresponding configuration in the constants file.</p> <p>To add a new camera:</p> <ol> <li>Mount the camera securely on the robot</li> <li>Measure the camera's position relative to robot center:<ul> <li>X: Distance forward from robot center (positive forward)</li> <li>Y: Distance left from robot center (positive left)</li> <li>Z: Height from ground (positive up)</li> </ul> </li> <li> <p>Measure the camera's rotation:</p> <ul> <li>Roll: Rotation around forward axis</li> <li>Pitch: Rotation around side axis</li> <li>Yaw: Rotation around vertical axis</li> </ul> </li> <li> <p>Add the camera configuration to PhotonvisionAprilTagSubsystemConstants:</p> </li> </ol> <pre><code>// Add camera object to CAMERAS array\npublic static final PhotonCamera[] CAMERAS = new PhotonCamera[] {\n    new PhotonCamera(\"FL_AprilTag\"),\n    new PhotonCamera(\"FM_AprilTag\"),\n    new PhotonCamera(\"NewCamera_AprilTag\")  // Add new camera\n};\n\n// Add corresponding offset to CAMERA_OFFSETS array\npublic static final Transform3d[] CAMERA_OFFSETS = new Transform3d[] {\n    // Existing cameras...\n    new Transform3d(\n        new Translation3d(x, y, z),\n        new Rotation3d(roll, pitch, yaw)\n    )\n};\n</code></pre>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#critical-tuning-parameters",
      "title": "Critical Tuning Parameters",
      "text": "<p>The system's performance depends heavily on proper tuning of several key parameters in PhotonvisionAprilTagSubsystemConstants:</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#vision-standard-deviations",
      "title": "Vision Standard Deviations",
      "text": "<pre><code>public static final Matrix&lt;N3, N1&gt; VISION_STD_DEVS\n</code></pre> <p>These values determine how much the system trusts vision measurements relative to odometry: - Lower values (0.001-0.1): High trust in vision - Higher values (0.5-1.0): Lower trust in vision - Start with higher values and decrease gradually while monitoring stability</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#pose-filter-parameters",
      "title": "Pose Filter Parameters",
      "text": "<pre><code>public static final double MAX_POSE_DEVIATION_METERS = 1.0;\npublic static final double MIN_TAG_WEIGHT = 0.3;\npublic static final double MAX_TAG_WEIGHT = 1.0;\n</code></pre> <p>These parameters control the outlier rejection and pose weighting system: - MAX_POSE_DEVIATION_METERS: Increase if legitimate poses are being rejected, decrease if false poses are being accepted - MIN/MAX_TAG_WEIGHT: Adjust the influence of single vs multi-tag detections</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#smoothing-parameters",
      "title": "Smoothing Parameters",
      "text": "<pre><code>public static final double POSITION_ALPHA = 0.05;\npublic static final double ROTATION_ALPHA = 0.05;\n</code></pre> <p>These control the trade-off between smoothness and responsiveness: - Lower values (0.01-0.1): Smoother but more latent - Higher values (0.1-0.3): More responsive but potentially jittery - Position and rotation can be tuned independently based on robot behavior</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#tuning-process",
      "title": "Tuning Process",
      "text": "<ol> <li> <p>Start with conservative values:</p> <ul> <li>High vision standard deviations</li> <li>High MAX_POSE_DEVIATION_METERS</li> <li>Low smoothing alphas</li> </ul> </li> <li> <p>Test basic functionality:</p> <ul> <li>Verify all cameras are detecting tags</li> <li>Check pose estimates against known positions</li> <li>Monitor for obvious outliers</li> </ul> </li> <li> <p>Iterative refinement:</p> <ul> <li>Gradually lower vision standard deviations while monitoring pose stability</li> <li>Adjust MAX_POSE_DEVIATION_METERS to balance outlier rejection</li> <li>Fine-tune smoothing alphas based on robot movement characteristics</li> </ul> </li> <li> <p>Validation:</p> <ul> <li>Test under competition conditions</li> <li>Verify performance during rapid movement</li> <li>Check reliability with partially obscured tags</li> </ul> </li> </ol> <p>Monitor the telemetry output during tuning to observe the effect of parameter changes on system performance. The field visualization and pose graphs are particularly useful for identifying issues during the tuning process.</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#vision-pipeline",
      "title": "Vision Pipeline",
      "text": ""
    },
    {
      "location": "library/subsystems/vision/apriltag/#data-acquisition-and-processing",
      "title": "Data Acquisition and Processing",
      "text": "<p>The vision pipeline begins in the periodic loop where each camera captures frames independently. The system processes these frames asynchronously to identify AprilTags using PhotonVision's detection algorithms. For each detected tag, the system calculates a camera-relative pose using PNP (Perspective-n-Point) algorithms.</p> <p>The raw camera data undergoes several processing stages:</p> <ol> <li>Tag Detection: Each camera processes its image to identify AprilTags in its field of view.</li> <li>Pose Calculation: The system calculates individual pose estimates based on the detected tags.</li> <li>Multi-Tag Fusion: When multiple tags are visible, their poses are combined for improved accuracy.</li> <li>Cross-Camera Fusion: Poses from different cameras are merged using weighted averaging.</li> </ol>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#pose-estimation-algorithm",
      "title": "Pose Estimation Algorithm",
      "text": "<p>The pose estimation process employs a sophisticated multi-stage algorithm:</p> <p>First, the system collects pose estimates from all available cameras. Each estimate includes information about the tags used for the calculation and their detection confidence. The system then applies an outlier rejection algorithm that compares poses against the median position, filtering out estimates that deviate significantly from the consensus.</p> <p>The remaining poses undergo a weighted averaging process. The weighting system considers several factors: - Number of visible tags (more tags increase weight) - Tag pose ambiguity (lower ambiguity increases weight) - Historical reliability of the camera</p> <p>The weighted averaging process uses exponential smoothing to reduce jitter while maintaining responsiveness. Position and rotation are smoothed independently with configurable smoothing factors.</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#outlier-rejection-system",
      "title": "Outlier Rejection System",
      "text": "<p>The outlier rejection system uses a statistical approach to identify and remove unreliable pose estimates. It calculates the median position of all estimates and measures the deviation of each estimate from this median. Estimates that exceed a configurable maximum deviation threshold are excluded from the final pose calculation.</p> <p>The system also considers the geometric consistency of poses when multiple tags are visible. If the relative positions of detected tags don't match their known field positions within tolerance, the system can identify and reject inconsistent measurements.</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#pose-smoothing-implementation",
      "title": "Pose Smoothing Implementation",
      "text": "<p>The pose smoothing system uses exponential smoothing with separate smoothing factors for position and rotation. This dual-factor approach allows fine-tuning of the smoothing behavior independently for translational and rotational movements.</p> <p>The smoothing algorithm maintains state between updates, storing the last smoothed pose. New pose estimates are combined with this historical data using the configurable alpha factors:</p> <pre><code>smoothed_value = alpha * new_value + (1 - alpha) * previous_smoothed_value\n</code></pre> <p>Lower alpha values produce smoother output but increase latency, while higher values provide more responsive updates but may introduce more jitter.</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#multi-camera-fusion",
      "title": "Multi-Camera Fusion",
      "text": "<p>The multi-camera fusion system combines pose estimates from different perspectives to improve accuracy and reliability. Each camera contributes to the final pose estimate based on its current view quality and historical reliability.</p> <p>The fusion process handles several challenges: - Temporal alignment of measurements from different cameras - Resolution of conflicting pose estimates - Handling of partial visibility conditions - Compensation for camera calibration uncertainties</p> <p>The system uses a weighted average approach that considers the number of tags visible to each camera and the geometric quality of the view (based on tag size and angle in the image).</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#integration-with-robot-odometry",
      "title": "Integration with Robot Odometry",
      "text": "<p>The subsystem integrates with the robot's odometry system through the SwerveDriveSubsystem interface. Vision measurements are added to the odometry system with appropriate uncertainty values, allowing the robot's state estimator to fuse vision data with other sensor inputs.</p> <p>The vision standard deviations are configurable through a covariance matrix, enabling fine-tuning of how much the system trusts vision measurements relative to other sensors.</p>"
    },
    {
      "location": "library/subsystems/vision/apriltag/#error-handling-and-robustness",
      "title": "Error Handling and Robustness",
      "text": "<p>The system implements comprehensive error handling to maintain operation under various failure conditions:</p> <ul> <li>Camera disconnections are detected and handled gracefully</li> <li>Individual camera failures don't compromise the entire system</li> <li>Network communication issues are managed through timeout mechanisms</li> <li>Invalid pose estimates are filtered out before they can affect robot behavior</li> </ul> <p>The system can continue operating with reduced accuracy when some cameras are unavailable, automatically adjusting its fusion algorithms to work with the remaining functional cameras.</p>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/",
      "title": "PhotonVision Object Detection Subsystem",
      "text": ""
    },
    {
      "location": "library/subsystems/vision/objectdetection/#overview",
      "title": "Overview",
      "text": "<p>The PhotonVision Object Detection System is a sophisticated computer vision subsystem designed for FRC robots. It provides real-time detection and tracking of game pieces and other objects using multiple PhotonVision cameras. The system maintains object persistence across frames, handles confidence decay, and provides comprehensive telemetry data for debugging and visualization.</p> <p>For AprilTag-specific vision features, see the AprilTag documentation.</p> <p>Related commands: - DriveFacingBestObject - PathfindToBestObject</p>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#core-components",
      "title": "Core Components",
      "text": ""
    },
    {
      "location": "library/subsystems/vision/objectdetection/#detectedobject-class",
      "title": "DetectedObject Class",
      "text": "<p>The DetectedObject class serves as the fundamental unit of object tracking. It encapsulates: - 3D pose information in field coordinates - Raw PhotonVision target data - Camera-to-robot transform data - Confidence tracking - Distance and angle calculations</p> <p>Each detected object maintains a confidence value that decays over time when the object is not actively detected, helping filter out spurious detections while maintaining object persistence when detection is briefly lost.</p>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#photonvisionobjectdetectionsubsystem",
      "title": "PhotonVisionObjectDetectionSubsystem",
      "text": "<p>This is the main subsystem class that orchestrates the entire detection system. Key responsibilities include: 1. Managing multiple PhotonVision cameras 2. Processing incoming vision data 3. Maintaining the list of detected objects 4. Handling object persistence and matching 5. Managing detection timers for filtering false positives</p> <p>The subsystem implements several sophisticated mechanisms: - Object Matching: Uses position-based matching to update existing objects rather than creating duplicates - Confidence Decay: Gradually reduces confidence in objects that haven't been seen recently - Detection Timers: Requires consistent detection over time before adding new objects - Multi-Camera Integration: Processes and combines data from multiple cameras</p>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#telemetry-system",
      "title": "Telemetry System",
      "text": "<p>The telemetry component provides comprehensive debugging and visualization capabilities: - Real-time field visualization - Per-camera statistics and controls - Object tracking visualization with tracer lines - Confidence and position data for detected objects - Toggle switches for enabling/disabling features</p>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#system-operation",
      "title": "System Operation",
      "text": ""
    },
    {
      "location": "library/subsystems/vision/objectdetection/#detection-pipeline",
      "title": "Detection Pipeline",
      "text": "<ol> <li> <p>Camera Processing</p> <ul> <li>Each enabled camera's latest frame is processed</li> <li>Raw targets are extracted from PhotonVision</li> <li>Targets are converted to field-relative 3D poses</li> </ul> </li> <li> <p>Object Management</p> <ul> <li>New detections are matched against existing objects</li> <li>Matched objects have their confidence refreshed</li> <li>Unmatched detections start a detection timer</li> <li>Objects passing the minimum detection time are added to tracking</li> </ul> </li> <li> <p>Maintenance</p> <ul> <li>Confidence values decay over time</li> <li>Stale objects are removed</li> <li>Detection timers are cleaned up</li> <li>Telemetry is updated</li> </ul> </li> </ol>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#object-persistence",
      "title": "Object Persistence",
      "text": "<p>The system uses several mechanisms to maintain reliable object tracking: - Position-based matching within a configurable tolerance - Confidence decay for graceful degradation - Detection timers to prevent false positives - Grace periods for temporary detection loss</p>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#configuration-and-tuning",
      "title": "Configuration and Tuning",
      "text": ""
    },
    {
      "location": "library/subsystems/vision/objectdetection/#constants-configuration",
      "title": "Constants Configuration",
      "text": "<p>Key parameters that need tuning in PhotonVisionObjectDetectionSubsystemConstants:</p> <ol> <li> <p>Camera Configuration</p> <ul> <li><code>CAMERAS</code>: Array of PhotonCamera objects</li> <li><code>CAMERA_OFFSETS</code>: 3D transforms from robot center to each camera    <code>java    new Transform3d(    new Translation3d(x, y, z),    new Rotation3d(roll, pitch, yaw)    )</code></li> </ul> </li> <li> <p>Detection Parameters</p> <ul> <li><code>INITIAL_CONFIDENCE</code> (0.99): Starting confidence for new detections</li> <li><code>CONFIDENCE_DECAY_RATE</code> (0.33): How quickly confidence decreases</li> <li><code>MIN_CONFIDENCE</code> (0.0): Threshold for removing objects</li> <li><code>POSITION_MATCH_TOLERANCE</code> (0.5m): Distance threshold for matching objects</li> <li><code>MINIMUM_DETECTION_TIME</code> (0.25s): Required consistent detection time</li> <li><code>TIMER_CLEANUP_GRACE_PERIOD</code> (5.0s): How long to maintain detection timers</li> </ul> </li> </ol>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#adding-new-cameras",
      "title": "Adding New Cameras",
      "text": "<p>To add a new camera: 1. Add a new PhotonCamera instance to the <code>CAMERAS</code> array:    <code>java    public static final PhotonCamera[] CAMERAS = new PhotonCamera[] {        new PhotonCamera(\"Camera1\"),        new PhotonCamera(\"Camera2\")  // New camera    };</code> 2. Add corresponding offset transform in <code>CAMERA_OFFSETS</code>:    <code>java    public static final Transform3d[] CAMERA_OFFSETS = new Transform3d[] {        existing_transform,        new Transform3d(            new Translation3d(x, y, z),            new Rotation3d(roll, pitch, yaw)        )    };</code></p>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#adding-new-game-elements",
      "title": "Adding New Game Elements",
      "text": "<p>To add new detectable objects: 1. Add new GameElement to <code>GAME_ELEMENTS</code> array:    <code>java    public static final GameElement[] GAME_ELEMENTS = new GameElement[] {        existing_element,        new GameElement(\"NewObject\", length, width, height)    };</code> 2. Ensure the array index matches the class ID from PhotonVision's model</p>"
    },
    {
      "location": "library/subsystems/vision/objectdetection/#best-practices",
      "title": "Best Practices",
      "text": "<ol> <li> <p>Camera Placement</p> <ul> <li>Mount cameras with clear fields of view</li> <li>Minimize exposure to bright lights and sun</li> <li>Ensure rigid mounting to prevent vibration</li> <li>Accurately measure and configure offsets</li> </ul> </li> <li> <p>Tuning</p> <ul> <li>Start with higher confidence decay rates</li> <li>Adjust position tolerance based on field size</li> <li>Tune minimum detection time based on frame rate</li> <li>Set appropriate grace periods for your game piece dynamics</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Enable only necessary cameras</li> <li>Use teleop-only mode when appropriate</li> <li>Monitor CPU usage and adjust parameters</li> <li>Clean up stale objects and timers regularly</li> </ul> </li> </ol>"
    }
  ]
}
